<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Differential Privacy for Beginners</title>

  <style>
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial;
      line-height: 1.7;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 16px;
      color: #111827;
    }
    h1, h2 {
      line-height: 1.25;
    }
    h1 {
      margin-bottom: 0.2em;
    }
    .subtitle {
      color: #6b7280;
      margin-bottom: 2em;
    }
    p {
      margin: 1em 0;
    }
    footer {
      margin-top: 4em;
      color: #6b7280;
      font-size: 0.95em;
    }
  </style>
</head>

<body>

<h1>Differential Privacy for Beginners</h1>
<p class="subtitle">
These notes reflect my personal understanding of differential privacy,
with the goal of explaining the core intuition to undergraduate and graduate students.
</p>

<h2>Why data privacy is harder than it looks</h2>

<p>
When people hear the phrase <em>data privacy</em>, the first idea that usually comes to mind
is simple: just remove names.
</p>

<p>
At an intuitive level, this sounds reasonable. If a dataset does not contain names or IDs,
how could it possibly point to a real person?
</p>

<p>
The problem is that real-world data does not behave like a clean spreadsheet from a textbook.
Real data is rich, messy, and full of patterns. And patterns are often enough to identify
someone — even when no explicit identifier is present.
</p>

<h2>Why removing names is not enough</h2>

<p>
Consider a dataset where each row corresponds to one individual.
Suppose we remove names and obvious identifiers, but keep attributes such as age,
ZIP code, gender, or timestamps.
</p>

<p>
Individually, none of these attributes seems sensitive.
Together, however, they can be highly distinctive.
In practice, combinations of seemingly harmless attributes often act like a fingerprint.
</p>

<p>
Once a record becomes unique, linking it to external information becomes surprisingly easy.
This is not a theoretical concern — it has happened repeatedly in real data releases,
including health records, mobility traces, and recommendation datasets.
</p>

<p>
The key takeaway is simple:
<strong>anonymity is not a property of a single column, but of the dataset as a whole.</strong>
</p>

<h2>When behavior becomes identifying</h2>

<p>
Another reason anonymization fails is that data often captures behavior over time.
Human behavior is highly structured and surprisingly consistent.
</p>

<p>
The places you visit, the times you are active, or the items you interact with
tend to form patterns that are difficult to hide.
Even partial auxiliary information can be enough to match these patterns
to a supposedly anonymous dataset.
</p>

<p>
This means that privacy failures do not require broken encryption or malicious insiders.
They can arise naturally from the structure of the data itself.
</p>

<h2>What if we never release the data?</h2>

<p>
A natural response is to avoid releasing datasets altogether.
Instead, analysts are allowed to submit queries and only receive aggregate answers.
</p>

<p>
At first, this seems safer. After all, a single number does not appear to reveal much
about any individual.
</p>

<p>
The problem is that queries can be carefully designed.
By asking multiple related questions and comparing the answers,
it is often possible to isolate the contribution of a single individual.
</p>

<p>
Even rules like “only answer queries with large counts” do not fully solve the problem.
Differences between aggregate answers can still leak sensitive information.
</p>

<h2>Why these failures matter</h2>

<p>
Across all these examples, privacy loss happens indirectly.
Information is not leaked because we explicitly reveal a secret,
but because released outputs allow someone to reason backwards.
</p>

<p>
Once we accept this, it becomes clear that privacy cannot be guaranteed
by ad-hoc anonymization or manual access control.
We need a formal notion of privacy that limits what can be inferred about
any single individual — even in the presence of auxiliary information.
</p>

<p>
This motivation leads naturally to differential privacy.
</p>

<footer>
<p>
This page is intentionally minimal. I plan to expand it gradually as my understanding
and explanations evolve.
</p>
</footer>

</body>
</html>
